{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from facenet_pytorch import InceptionResnetV1, MTCNN\n",
    "from PIL import Image, ImageDraw\n",
    "import os\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomFaceNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CustomFaceNet, self).__init__()\n",
    "        self.facenet = InceptionResnetV1(pretrained='vggface2', classify=False).eval()\n",
    "\n",
    "        for param in self.facenet.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.logits = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.facenet(x)\n",
    "        x = self.logits(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thanh\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "transforms = v2.Compose([\n",
    "    v2.RandomHorizontalFlip(),\n",
    "    v2.RandomRotation(15),\n",
    "    v2.RandomResizedCrop(160, scale=(0.8, 1.0)),\n",
    "    v2.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "    v2.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n",
    "    v2.RandomGrayscale(p=0.1),\n",
    "    v2.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    v2.Resize(160),\n",
    "    v2.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "\n",
    "dataset = datasets.ImageFolder('Face Dataset/Train', transform=transforms)\n",
    "dataloader = DataLoader(dataset, shuffle=True)\n",
    "num_classes = len(dataset.classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Epoch: 0, Loss: 2.1046134531497955\n",
      "Epoch: 1, Loss: 2.055146872997284\n",
      "Epoch: 2, Loss: 2.022596761584282\n",
      "Epoch: 3, Loss: 1.995435208082199\n",
      "Epoch: 4, Loss: 1.9662160873413086\n",
      "Epoch: 5, Loss: 1.932995468378067\n",
      "Epoch: 6, Loss: 1.9073055535554886\n",
      "Epoch: 7, Loss: 1.8880493193864822\n",
      "Epoch: 8, Loss: 1.852277159690857\n",
      "Epoch: 9, Loss: 1.8282759338617325\n",
      "Epoch: 10, Loss: 1.7895135432481766\n",
      "Epoch: 11, Loss: 1.7880786210298538\n",
      "Epoch: 12, Loss: 1.7509153634309769\n",
      "Epoch: 13, Loss: 1.7197094857692719\n",
      "Epoch: 14, Loss: 1.6914309412240982\n",
      "Epoch: 15, Loss: 1.6747811883687973\n",
      "Epoch: 16, Loss: 1.6522886753082275\n",
      "Epoch: 17, Loss: 1.624023199081421\n",
      "Epoch: 18, Loss: 1.5947152823209763\n",
      "Epoch: 19, Loss: 1.5722721964120865\n",
      "Epoch: 20, Loss: 1.5378143340349197\n",
      "Epoch: 21, Loss: 1.5400908589363098\n",
      "Epoch: 22, Loss: 1.5178442746400833\n",
      "Epoch: 23, Loss: 1.4991628229618073\n",
      "Epoch: 24, Loss: 1.4636926352977753\n",
      "Epoch: 25, Loss: 1.4329242259263992\n",
      "Epoch: 26, Loss: 1.3724967688322067\n",
      "Epoch: 27, Loss: 1.4071479588747025\n",
      "Epoch: 28, Loss: 1.4087913930416107\n",
      "Epoch: 29, Loss: 1.3511333614587784\n",
      "Epoch: 30, Loss: 1.4077510833740234\n",
      "Epoch: 31, Loss: 1.29966302216053\n",
      "Epoch: 32, Loss: 1.2861304432153702\n",
      "Epoch: 33, Loss: 1.3083494007587433\n",
      "Epoch: 34, Loss: 1.2620589137077332\n",
      "Epoch: 35, Loss: 1.2632746696472168\n",
      "Epoch: 36, Loss: 1.201019138097763\n",
      "Epoch: 37, Loss: 1.180464044213295\n",
      "Epoch: 38, Loss: 1.1765613555908203\n",
      "Epoch: 39, Loss: 1.1227715760469437\n",
      "Epoch: 40, Loss: 1.0995278805494308\n",
      "Epoch: 41, Loss: 1.1265606805682182\n",
      "Epoch: 42, Loss: 1.0956088528037071\n",
      "Epoch: 43, Loss: 1.0419850498437881\n",
      "Epoch: 44, Loss: 1.0646666511893272\n",
      "Epoch: 45, Loss: 1.0224458575248718\n",
      "Epoch: 46, Loss: 1.0330475196242332\n",
      "Epoch: 47, Loss: 1.0486950501799583\n",
      "Epoch: 48, Loss: 0.9483800455927849\n",
      "Epoch: 49, Loss: 1.0639297366142273\n",
      "Epoch: 50, Loss: 0.9688304141163826\n",
      "Epoch: 51, Loss: 0.9880447834730148\n",
      "Epoch: 52, Loss: 0.9173259809613228\n",
      "Epoch: 53, Loss: 0.9139644205570221\n",
      "Epoch: 54, Loss: 0.9154965877532959\n",
      "Epoch: 55, Loss: 0.9531077668070793\n",
      "Epoch: 56, Loss: 0.8784420341253281\n",
      "Epoch: 57, Loss: 0.8761032670736313\n",
      "Epoch: 58, Loss: 0.841238223016262\n",
      "Epoch: 59, Loss: 0.7972357049584389\n",
      "Epoch: 60, Loss: 0.8676899448037148\n",
      "Epoch: 61, Loss: 0.8470751866698265\n",
      "Epoch: 62, Loss: 0.7631315961480141\n",
      "Epoch: 63, Loss: 0.7786062359809875\n",
      "Epoch: 64, Loss: 0.7579202204942703\n",
      "Epoch: 65, Loss: 0.7509771436452866\n",
      "Epoch: 66, Loss: 0.7671825215220451\n",
      "Epoch: 67, Loss: 0.7842900678515434\n",
      "Epoch: 68, Loss: 0.738843597471714\n",
      "Epoch: 69, Loss: 0.6916518881917\n",
      "Epoch: 70, Loss: 0.7565162107348442\n",
      "Epoch: 71, Loss: 0.6935405433177948\n",
      "Epoch: 72, Loss: 0.6892328858375549\n",
      "Epoch: 73, Loss: 0.6667339019477367\n",
      "Epoch: 74, Loss: 0.6422657519578934\n",
      "Epoch: 75, Loss: 0.68878073990345\n",
      "Epoch: 76, Loss: 0.6317602284252644\n",
      "Epoch: 77, Loss: 0.6369878388941288\n",
      "Epoch: 78, Loss: 0.6338651664555073\n",
      "Epoch: 79, Loss: 0.5941198170185089\n",
      "Epoch: 80, Loss: 0.6604022197425365\n",
      "Epoch: 81, Loss: 0.5972614772617817\n",
      "Epoch: 82, Loss: 0.5821584388613701\n",
      "Epoch: 83, Loss: 0.5629144832491875\n",
      "Epoch: 84, Loss: 0.5744289234280586\n",
      "Epoch: 85, Loss: 0.5271086432039738\n",
      "Epoch: 86, Loss: 0.5974229648709297\n",
      "Epoch: 87, Loss: 0.6205420941114426\n",
      "Epoch: 88, Loss: 0.5296349041163921\n",
      "Epoch: 89, Loss: 0.5784962587058544\n",
      "Epoch: 90, Loss: 0.5624621026217937\n",
      "Epoch: 91, Loss: 0.5093002468347549\n",
      "Epoch: 92, Loss: 0.5452754534780979\n",
      "Epoch: 93, Loss: 0.5055934898555279\n",
      "Epoch: 94, Loss: 0.4723585657775402\n",
      "Epoch: 95, Loss: 0.4867859371006489\n",
      "Epoch: 96, Loss: 0.4782574996352196\n",
      "Epoch: 97, Loss: 0.5143496841192245\n",
      "Epoch: 98, Loss: 0.5014865212142467\n",
      "Epoch: 99, Loss: 0.5375410653650761\n"
     ]
    }
   ],
   "source": [
    "model = CustomFaceNet(num_classes)\n",
    "\n",
    "print(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(100):\n",
    "    running_loss = 0.0\n",
    "    for i, (images, labels) in enumerate(dataloader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch: {epoch}, Loss: {running_loss/len(dataloader)}')\n",
    "torch.save(model.state_dict(), 'models/facenet.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'E:\\\\Project\\\\Face Verification\\\\Face Dataset\\\\Test\\\\Tran Quang Thanh'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 44\u001b[0m\n\u001b[0;32m     41\u001b[0m             draw\u001b[38;5;241m.\u001b[39mtext((box[\u001b[38;5;241m0\u001b[39m], box[\u001b[38;5;241m1\u001b[39m]), name, font\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, fill\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m image\n\u001b[1;32m---> 44\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mE:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mProject\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mFace Verification\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mFace Dataset\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mTest\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mTran Quang Thanh\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     45\u001b[0m image \u001b[38;5;241m=\u001b[39m recognize(image)\n\u001b[0;32m     46\u001b[0m display\u001b[38;5;241m.\u001b[39mdisplay(image)\n",
      "File \u001b[1;32mc:\\Users\\thanh\\AppData\\Local\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:3247\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3244\u001b[0m     filename \u001b[38;5;241m=\u001b[39m fp\n\u001b[0;32m   3246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[1;32m-> 3247\u001b[0m     fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   3248\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   3250\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'E:\\\\Project\\\\Face Verification\\\\Face Dataset\\\\Test\\\\Tran Quang Thanh'"
     ]
    }
   ],
   "source": [
    "mtcnn = MTCNN(keep_all=True, device=device)\n",
    "model = CustomFaceNet(num_classes)\n",
    "model.load_state_dict(torch.load('models/facenet.pth'))\n",
    "model.to(device)\n",
    "\n",
    "def draw_boxes(image, boxes):\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    for box in boxes:\n",
    "        draw.rectangle(box.tolist(), outline=(255, 0, 0), width=6)\n",
    "    return image\n",
    "\n",
    "def get_faces(image):\n",
    "    boxes, _ = mtcnn.detect(image)\n",
    "    if boxes is not None:\n",
    "        faces = [image.crop(box) for box in boxes]\n",
    "        return faces, boxes\n",
    "    return None, None\n",
    "\n",
    "def get_embedding(face, transforms):\n",
    "    face = transforms(face).unsqueeze(0).to(device)\n",
    "    embedding = model(face)\n",
    "    return embedding\n",
    "\n",
    "def get_prediction(embedding):\n",
    "    with torch.no_grad():\n",
    "        prediction = torch.argmax(embedding, dim=1).item()\n",
    "    return prediction\n",
    "\n",
    "def get_name(prediction):\n",
    "    return dataset.classes[prediction]\n",
    "\n",
    "def recognize(image):\n",
    "    faces, boxes = get_faces(image)\n",
    "    if faces is not None:\n",
    "        for face, box in zip(faces, boxes):\n",
    "            embedding = get_embedding(face, transforms)\n",
    "            prediction = get_prediction(embedding)\n",
    "            name = get_name(prediction)\n",
    "            image = draw_boxes(image, [box])\n",
    "            draw = ImageDraw.Draw(image)\n",
    "            draw.text((box[0], box[1]), name, font=None, fill=(255, 0, 0))\n",
    "    return image\n",
    "\n",
    "image = Image.open(r'E:\\Project\\Face Verification\\Face Dataset\\Test\\Tran Quang Thanh\\1603945b-5cf4-4082-8e53-6ed522bec447.jfif')\n",
    "image = recognize(image)\n",
    "display.display(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
