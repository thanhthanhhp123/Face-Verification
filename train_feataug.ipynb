{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from facenet_pytorch import MTCNN, InceptionResnetV1, fixed_image_standardization\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision import transforms as v2\n",
    "from torchvision.datasets import ImageFolder\n",
    "import warnings\n",
    "\n",
    "from src.utils import *\n",
    "from src.modules import *\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = v2.Compose(\n",
    "    [v2.RandomHorizontalFlip(),\n",
    "        v2.RandomRotation(15),\n",
    "        v2.RandomResizedCrop(160, scale=(0.8, 1.0)),\n",
    "        v2.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "        v2.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n",
    "        v2.RandomGrayscale(p=0.1),\n",
    "        v2.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "        v2.Resize((160, 160)),\n",
    "     v2.ToTensor(),\n",
    "     v2.Lambda(lambda x: x + torch.randn_like(x) * 0.05),\n",
    "     fixed_image_standardization]\n",
    ")\n",
    "\n",
    "dataset = ImageFolder(\"FaceDataset/Train_cropped\", transform=transforms)\n",
    "loader = DataLoader(dataset, batch_size=6, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "facenet_model = InceptionResnetV1(pretrained=\"casia-webface\", classify=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1792, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "feature_agg = NetworkFeatureAggregator(facenet_model, ['block8'], device = device, train_backbone=True)\n",
    "features = feature_agg(torch.rand(64, 3, 160, 160).to(device))\n",
    "print(features['block8'].shape)\n",
    "features = [features[layer] for layer in ['block8']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_layer = AugmentationLayer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceNetClassifierWithAugFMap(nn.Module):\n",
    "    def __init__(self, num_classes, aug_layer = None):\n",
    "        super(FaceNetClassifierWithAugFMap, self).__init__()\n",
    "        self.extractor = feature_agg\n",
    "        self.aug_layer = None if aug_layer is None else aug_layer\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1792, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.extractor(x)['block8']\n",
    "        if self.aug_layer is not None:\n",
    "            features = self.aug_layer(features)\n",
    "        # features = features.view(features.size(0), -1)\n",
    "        x = self.fc(features)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "facenet = FaceNetClassifierWithAugFMap(num_classes=len(dataset.classes), aug_layer=AugmentationLayer()).to(device)\n",
    "\n",
    "epochs = 100\n",
    "facenet_params = list(facenet.extractor.parameters())\n",
    "classifier_params = list(facenet.fc.parameters())\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': facenet_params, 'lr': 1e-4},\n",
    "    {'params': classifier_params, 'lr': 1e-3}\n",
    "])\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "early_stopping = EarlyStopping(patience=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss improved to 3.022618.\n",
      "Accuracy did not improve. Counter: 1/5\n",
      "Epoch: 0 | Train Loss: 3.0226, Train Accuracy: 0.0000\n",
      "Loss improved to 2.888305.\n",
      "Accuracy improved to 0.300000.\n",
      "Epoch: 1 | Train Loss: 2.8883, Train Accuracy: 0.3000\n",
      "Loss improved to 2.697709.\n",
      "Accuracy improved to 0.550000.\n",
      "Epoch: 2 | Train Loss: 2.6977, Train Accuracy: 0.5500\n",
      "Loss improved to 2.559051.\n",
      "Accuracy improved to 0.700000.\n",
      "Epoch: 3 | Train Loss: 2.5591, Train Accuracy: 0.7000\n",
      "Loss improved to 2.493298.\n",
      "Accuracy improved to 0.850000.\n",
      "Epoch: 4 | Train Loss: 2.4933, Train Accuracy: 0.8500\n",
      "Loss improved to 2.294947.\n",
      "Accuracy improved to 1.000000.\n",
      "Epoch: 5 | Train Loss: 2.2949, Train Accuracy: 1.0000\n",
      "Loss improved to 2.265279.\n",
      "Accuracy did not improve. Counter: 1/5\n",
      "Epoch: 6 | Train Loss: 2.2653, Train Accuracy: 0.8500\n",
      "Loss improved to 2.108851.\n",
      "Accuracy did not improve. Counter: 1/5\n",
      "Epoch: 7 | Train Loss: 2.1089, Train Accuracy: 1.0000\n",
      "Loss improved to 1.934183.\n",
      "Accuracy did not improve. Counter: 1/5\n",
      "Epoch: 8 | Train Loss: 1.9342, Train Accuracy: 1.0000\n",
      "Loss improved to 1.829243.\n",
      "Accuracy did not improve. Counter: 1/5\n",
      "Epoch: 9 | Train Loss: 1.8292, Train Accuracy: 1.0000\n",
      "Loss improved to 1.734147.\n",
      "Accuracy did not improve. Counter: 1/5\n",
      "Epoch: 10 | Train Loss: 1.7341, Train Accuracy: 1.0000\n",
      "Loss improved to 1.683826.\n",
      "Accuracy did not improve. Counter: 1/5\n",
      "Epoch: 11 | Train Loss: 1.6838, Train Accuracy: 1.0000\n",
      "Loss improved to 1.605514.\n",
      "Accuracy did not improve. Counter: 1/5\n",
      "Epoch: 12 | Train Loss: 1.6055, Train Accuracy: 1.0000\n",
      "Loss improved to 1.486567.\n",
      "Accuracy did not improve. Counter: 1/5\n",
      "Epoch: 13 | Train Loss: 1.4866, Train Accuracy: 1.0000\n",
      "Loss improved to 1.435485.\n",
      "Accuracy did not improve. Counter: 1/5\n",
      "Epoch: 14 | Train Loss: 1.4355, Train Accuracy: 1.0000\n",
      "Loss did not improve. Counter: 2/5\n",
      "Accuracy did not improve. Counter: 3/5\n",
      "Epoch: 15 | Train Loss: 1.4657, Train Accuracy: 0.9500\n",
      "Loss improved to 1.219930.\n",
      "Accuracy did not improve. Counter: 1/5\n",
      "Epoch: 16 | Train Loss: 1.2199, Train Accuracy: 1.0000\n",
      "Loss did not improve. Counter: 2/5\n",
      "Accuracy did not improve. Counter: 3/5\n",
      "Epoch: 17 | Train Loss: 1.3073, Train Accuracy: 0.9000\n",
      "Loss improved to 1.119602.\n",
      "Accuracy did not improve. Counter: 1/5\n",
      "Epoch: 18 | Train Loss: 1.1196, Train Accuracy: 1.0000\n",
      "Loss improved to 1.098179.\n",
      "Accuracy did not improve. Counter: 1/5\n",
      "Epoch: 19 | Train Loss: 1.0982, Train Accuracy: 1.0000\n",
      "Loss improved to 1.044283.\n",
      "Accuracy did not improve. Counter: 1/5\n",
      "Epoch: 20 | Train Loss: 1.0443, Train Accuracy: 1.0000\n",
      "Loss improved to 1.006971.\n",
      "Accuracy did not improve. Counter: 1/5\n",
      "Epoch: 21 | Train Loss: 1.0070, Train Accuracy: 1.0000\n",
      "Loss improved to 0.777964.\n",
      "Accuracy did not improve. Counter: 1/5\n",
      "Epoch: 22 | Train Loss: 0.7780, Train Accuracy: 1.0000\n",
      "Loss did not improve. Counter: 2/5\n",
      "Accuracy did not improve. Counter: 3/5\n",
      "Epoch: 23 | Train Loss: 0.8834, Train Accuracy: 1.0000\n",
      "Loss did not improve. Counter: 4/5\n",
      "Accuracy did not improve. Counter: 5/5\n",
      "Early stopping triggered!\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "best_train_loss = float('inf')\n",
    "best_train_acc = 0.0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    facenet.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for x, y in (loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred = facenet(x)\n",
    "\n",
    "        loss = criterion(y_pred, y)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * y.size(0) \n",
    "        _, predicted = torch.max(y_pred, 1) \n",
    "        correct += (predicted == y).sum().item()\n",
    "        total += y.size(0)\n",
    "    train_loss /= total \n",
    "    train_acc = correct / total \n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    early_stopping(train_loss, train_acc)\n",
    "    if train_loss < best_train_loss:\n",
    "        best_val_loss = train_loss\n",
    "        best_val_acc = train_acc\n",
    "        torch.save({\n",
    "            'facenet_state_dict': facenet.state_dict(),\n",
    "        }, 'models/best_model.pth')\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "    torch.save({\n",
    "        'facenet_state_dict': facenet.state_dict(),\n",
    "    }, 'models/last_model.pth')\n",
    "    torch.save({\n",
    "        \"losses\": train_losses,\n",
    "        \"accuracies\": train_accuracies,\n",
    "    }, 'models/train_metrics.pth')\n",
    "    print(f\"Epoch: {epoch} | Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transforms = v2.Compose(\n",
    "    [v2.Resize((160, 160)),\n",
    "     v2.ToTensor(),\n",
    "     fixed_image_standardization]\n",
    ")\n",
    "\n",
    "test_dataset = ImageFolder(\"FaceDataset/Test_cropped\", transform=transforms)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc: 81.23916811091854\n"
     ]
    }
   ],
   "source": [
    "test_acc = 0.0\n",
    "test_loss = 0.0\n",
    "\n",
    "checkpoint = torch.load(r'models\\best_model.pth')\n",
    "facenet.load_state_dict(checkpoint['facenet_state_dict'])\n",
    "\n",
    "facenet.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "for x, y in test_loader:\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    y_pred = facenet(x)\n",
    "    _, predicted = torch.max(y_pred, 1)\n",
    "    correct += (predicted == y).sum().item()\n",
    "    total += y.size(0)\n",
    "test_acc = correct / total\n",
    "print(f\"Test Acc: {test_acc*100}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
